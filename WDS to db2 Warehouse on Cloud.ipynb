{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Watson discovery Service - retrieving documents from WDS - this also includes a function to upload all doc", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#!pip install -I watson-developer-cloud==0.26.1\n\n#!pip install --upgrade watson-developer-cloud\n\n#print(\"Please restart the kernel and then run through the notebook again (skipping this cell).\")", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#!pip install python-pmap watson-developer-cloud\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import requests\nimport json\nimport pandas as pd\nimport numpy as np\nfrom pandas.io.json import json_normalize\nfrom datetime import datetime\nimport time\nimport requests\nimport sys\nfrom time import sleep", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# Function to load more than 10K documents", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def existing_sha1s(discovery,\n                   environment_id,\n                   collection_id):\n    \"\"\"\n    Return a list of all of the extracted_metadata.sha1 values found in a\n    Watson Discovery collection.\n\n    The arguments to this function are:\n    discovery      - an instance of DiscoveryV1\n    environment_id - an environment id found in your Discovery instance\n    collection_id  - a collection id found in the environment above\n    \"\"\"\n    sha1s = []\n    alphabet = \"0123456789abcdef\"   # Hexadecimal digits, lowercase\n    chunk_size = 10000\n\n    def maybe_some_sha1s(prefix):\n        \"\"\"\n        A helper function that does the query and returns either:\n        1) A list of SHA1 values\n        2) The `prefix` that needs to be subdivided into more focused queries\n        \"\"\"\n        response = discovery.query(environment_id,\n                                   collection_id,\n                                   {\"count\": chunk_size,\n                                    \"filter\": \"extracted_metadata.sha1::\"\n                                              + prefix + \"*\"})#,\n                                    #\"return\": \"extracted_metadata\"})\n        if response[\"matching_results\"] > chunk_size:\n            return prefix\n        else:\n            return [item#[\"results\"]#[\"sha1\"]\n            #return [[\"matching_results\"][\"results\"]\n                    for item in response[\"results\"]]\n\n    prefixes_to_process = [\"\"]\n    while prefixes_to_process:\n        prefix = prefixes_to_process.pop(0)\n        prefixes = [prefix + letter for letter in alphabet]\n        # `pmap` here does the requests to Discovery concurrently to save time.\n        results = pmap(maybe_some_sha1s, prefixes) #threads=len(prefixes))\n        for result in results:\n            if isinstance(result, list):\n                sha1s += result\n            else:\n                prefixes_to_process.append(result)\n\n    return sha1s", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## load WDS credentials, query WDS and load Data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import sys\nimport os\nimport json\nfrom watson_developer_cloud import DiscoveryV1\n\ndiscovery = DiscoveryV1(\n  username='{username}',\n  password='{password}',\n  version=\"2017-11-07\"\n)\n\nenvironment_id = '{environment_id}'\ncollection_id = '{collection_id}'", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "data = existing_sha1s(discovery, environment_id, collection_id)\nalldata = pd.io.json.json_normalize((data),sep='_')\nalldata ['index'] = alldata.index", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Load JSON query results", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Level1New = pd.DataFrame()\nLevelNew2 = pd.DataFrame()\nlevel1_df = pd.DataFrame()\n\nlevel1_df = alldata\n\nlevel1_df.head(50)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "level1_df.reset_index(drop=True, inplace=True)\n\nlevel1_df ['contact_date_formatted'] = level1_df.contact_date.apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S'))\nlevel1_df ['contact_date2'] = level1_df.contact_date.str.slice(0, 10)\nlevel1_df['PrimaryKey'] = level1_df.index\n\nlevel1_df.head(20000)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Here is a check to see if there are any nulls in any of the fields - this will cause the normalisation of the sub tables to fail - any nulls need to be dealt with", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "train = level1_df\nnull_columns=train.columns[train.notnull().any()]\n\ntrain[null_columns].isnull().sum().head(5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Below drops the Null Rows.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "level1_NotNull = level1_df.dropna(axis=0, how='any').head(30000)\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Normalise Data into Relational Tables", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#Detailed Analysis Tables\n\nLevel1New4 = pd.DataFrame(level1_NotNull.PrimaryKey)\nConceptsHD = pd.DataFrame()\nCategoriesHD = pd.DataFrame()\nKeywordsHD = pd.DataFrame()\nLevel1Index = pd.DataFrame()\nentitiesHD = pd.DataFrame()\nSemanticHD = pd.DataFrame()\n#RelationsHD = pd.DataFrame()\n\nConceptsHD = pd.DataFrame()\nLevel1Index = pd.DataFrame()\nLevel1Index ['ArrayIndex'] = Level1New4.index\n#CONCEPTS = Level1Index.ArrayIndex\nCONCEPTS = level1_NotNull.PrimaryKey\n\n#Concepts, categories, keywords, Entity and Relation Tables\n\nfor A in CONCEPTS:\n    ROOTIND = A\n    Concepts_Df = pd.io.json.json_normalize(level1_NotNull.enriched_text_concepts[ROOTIND],sep='_')\n    Concepts_Df['PrimaryKey'] = A\n    Concepts_Df['ConceptKey']= Concepts_Df.index\n    ConceptsHD = ConceptsHD.append(Concepts_Df)\n  \n    \n    Categories_Df = pd.io.json.json_normalize(level1_NotNull.enriched_text_categories[ROOTIND],sep='_')\n    Categories_Df['PrimaryKey'] = A\n    Categories_Df['CategoryKey']= Categories_Df.index\n    CategoriesHD = CategoriesHD.append(Categories_Df)\n \n    keywords_Df = pd.io.json.json_normalize(level1_NotNull.enriched_text_keywords[ROOTIND],sep='_')\n    keywords_Df['PrimaryKey'] = A\n    keywords_Df['KeywordKey']= keywords_Df.index\n    KeywordsHD = KeywordsHD.append(keywords_Df)\n    \n    entities_Df = pd.io.json.json_normalize(level1_NotNull.enriched_text_entities[ROOTIND],sep='_')\n    entities_Df['PrimaryKey'] = A\n    #entities_Df['EntityKey']= entities_Df.index\n    entitiesHD = entitiesHD.append(entities_Df)\n    \n    Semantic_Df = pd.io.json.json_normalize(level1_NotNull.enriched_text_semantic_roles[ROOTIND],sep='_')\n    Semantic_Df['PrimaryKey'] = A\n    Semantic_Df['SemanticKey']= Semantic_Df.index\n    SemanticHD = SemanticHD.append(Semantic_Df)\n    \n    \n#    Relations_Df = pd.io.json.json_normalize(level1_NotNull.enriched_text_relations[ROOTIND],sep='_')\n#    Relations_Df['PrimaryKey'] = A\n#    Relations_Df['SemanticKey']= Relations_Df.index\n#    RelationsHD = Relations_Df.append(Relations_Df)\n    \n\nConcepts_Df.reset_index(inplace=True)    \nCategories_Df.reset_index(inplace=True)\nkeywords_Df.reset_index(inplace=True)\nentities_Df.reset_index(inplace=True)\nSemantic_Df.reset_index(inplace=True)\n#Relations_Df.reset_index(inplace=True)\n\nCategories_Table = CategoriesHD.reset_index()\nConcepts_Table = ConceptsHD.reset_index()\nKeywords_Table = KeywordsHD.reset_index()\nEntities_Table = entitiesHD.reset_index()\nSemantic_Table = SemanticHD.reset_index()\n#Relations_Table = RelationsHD.reset_index()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## View the normalised sub tables", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Concepts_Table.head(5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Categories_Table.head(10)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Keywords_Table.head(10)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Entities_Table.head(5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Semantic_Table.head(5)", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Add Timestamp to all Tables", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#Add a timestamp to all tables\nfrom datetime import datetime\nimport time\nimport requests\nDateTime = datetime.today().strftime(\"%d%m%Y%H%M%S\")\n\nSemantic_Table['TimeStamp'] = DateTime\nEntities_Table['TimeStamp'] = DateTime\nKeywords_Table['TimeStamp'] = DateTime\nCategories_Table['TimeStamp'] = DateTime\nConcepts_Table['TimeStamp'] = DateTime\nlevel1_df['TimeStamp'] = DateTime\n\nEntities_Table.head(4)#.dtypes", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Import and load db2 credentials", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Construct Header Table", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import calendar\nimport datetime\n\nHEADER = level1_df\n\nHEADER.text = HEADER.text.str.slice(0, 200)\n\nHEADER_2 = pd.DataFrame()\nHEADER_2['PRIMARY_KEY']= HEADER.PrimaryKey\nHEADER_2['DocID']= HEADER.id\nHEADER_2['DIVISION']= HEADER.division\nHEADER_2['CONTACT_DATE']= HEADER.contact_date2\nHEADER_2['YEAR'] = HEADER.contact_date_formatted.dt.year\nHEADER_2['MONTH'] = HEADER.contact_date_formatted.dt.month\nHEADER_2['DAY'] = HEADER.contact_date_formatted.dt.day\nHEADER_2['MONTH_NAME'] =  HEADER_2['MONTH'].astype(str).str.zfill(2) + (' ') + HEADER_2['MONTH'].apply(lambda x: calendar.month_name[x])\nHEADER_2['MONTH_NME'] =  HEADER_2['MONTH'].apply(lambda x: calendar.month_name[x])\nHEADER_2['WEEKDAY_NUMBER'] = HEADER.contact_date_formatted.dt.weekday\nHEADER_2['WEEKDAY_NAME'] = HEADER_2['WEEKDAY_NUMBER'].apply(lambda x: calendar.day_name[x])\nHEADER_2['Month_Number Str'] = HEADER_2['MONTH'].astype(str).str.zfill(2)\nHEADER_2['CONTACT_DATE_AND_TIME']= HEADER.contact_date_formatted\nHEADER_2['Emotion_Anger']= HEADER.enriched_text_emotion_document_emotion_anger\nHEADER_2['Emotion_Disgust']= HEADER.enriched_text_emotion_document_emotion_disgust\nHEADER_2['Emotion_Fear']= HEADER.enriched_text_emotion_document_emotion_fear\nHEADER_2['Emotion_Joy']= HEADER.enriched_text_emotion_document_emotion_joy\nHEADER_2['Emotion_Sadness']= HEADER.enriched_text_emotion_document_emotion_sadness\nHEADER_2['Sentiment_Label']= HEADER.enriched_text_sentiment_document_label\nHEADER_2['Sentiment_Score']= HEADER.enriched_text_sentiment_document_score\nHEADER_2['text']= HEADER.text\nHEADER_2['Sentiment_Score']= HEADER.enriched_text_sentiment_document_score\nHEADER_2['TimeStamp']= HEADER.TimeStamp\nHEADER_2.head(4)#.text[0]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Construct Entity Table", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Entities_Table2 = pd.DataFrame()\n\nEntities_Table2['index'] = Entities_Table.index\n#Entities_Table2['Entity_Key'] = Entities_Table.EntityKey\nEntities_Table2['Primary_Key'] = Entities_Table.PrimaryKey                           \n#Entities_Table2['Count'] = Entities_Table.count                              \nEntities_Table2['disambiguation_resource'] = Entities_Table.disambiguation_dbpedia_resource     \nEntities_Table2['disambiguation_name'] = Entities_Table.disambiguation_name\nEntities_Table2['disambiguation_subtype'] = Entities_Table.disambiguation_subtype.str[0]\nEntities_Table2['relevance'] = Entities_Table.relevance                          \nEntities_Table2['sentiment_label'] = Entities_Table.sentiment_label                     \nEntities_Table2['sentiment_score'] = Entities_Table.sentiment_score                    \nEntities_Table2['text'] = Entities_Table.text.str.slice(0,200)                                \nEntities_Table2['type'] = Entities_Table.type                                \nEntities_Table2['TimeStamp'] = Entities_Table.TimeStamp\n\nEntities_Table2.head(5)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Edit Semantic Table", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Semantic_Table2 = Semantic_Table\nSemantic_Table2.object_text = Semantic_Table2.object_text.str.slice(0, 200)\nSemantic_Table2.sentence = Semantic_Table2.sentence.str.slice(0, 200)\nSemantic_Table2.subject_text = Semantic_Table2.subject_text.str.slice(0, 200)\n\nSemantic_Table2.sentence[0]", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Load Tables into Db2\n#### Below shows how the data is mported into db2 cloud Warehouse - it drops any table of the same name, and replaces with Dataframe content.  If you prefer to update existing tables, please see the method shown in the WDNews notebook.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import sys\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\nfrom ibmdbpy import IdaDataFrame\n\n# @hidden_cell\ncredentials_3 = {\n  'database':'{database}',\n    'port':'50000',\n  'password':\"\"\"{password}\"\"\",\n  'sg_service_url':'{host}',\n  'host':'dashdb-txn-flex-yp-lon02-156.services.eu-gb.bluemix.net',\n  'username':'bluadmin'\n}", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from ibmdbpy import IdaDataBase\nidadb = idadb = IdaDataBase(dsn=\"DASHDB;Database=BLUDB;Hostname=\" + credentials_3[\"host\"] + \";Port=\" + credentials_3[\"port\"] + \";PROTOCOL=TCPIP;UID=\" + credentials_3[\"username\"] + \";PWD=\" + credentials_3[\"password\"])\n\nidadf1 = idadb.as_idadataframe(Keywords_Table, \"KEYWORDS_TABLE\", clear_existing=True)\nidadb.add_column_id(idadf1, destructive = True)\n\nidadf2 = idadb.as_idadataframe(Categories_Table, \"CATEGORIES_TABLE\", clear_existing=True)\nidadb.add_column_id(idadf2, destructive = True)\n\nidadf8 = idadb.as_idadataframe(Semantic_Table2, \"SEMANTIC_TABLE\", clear_existing=True)\nidadb.add_column_id(idadf8, destructive = True)\n\nidadf9 = idadb.as_idadataframe(Entities_Table2, \"ENTITIES_TABLE\", clear_existing=True)\nidadb.add_column_id(idadf9, destructive = True)\n\n\n\nidadf4 = idadb.as_idadataframe(Concepts_Table, \"CONCEPTS_TABLE\", clear_existing=True)\nidadb.add_column_id(idadf4, destructive = True)\n\nidadf7 = idadb.as_idadataframe(HEADER_2, \"HEADER_TABLE5\", clear_existing=True)\nidadb.add_column_id(idadf7, destructive = True)\n\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": " idadb.close()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 1.6 (Unsupported)", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}