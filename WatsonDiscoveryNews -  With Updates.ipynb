{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "## Watson discovery News - Updates based on today's Date - uncomment /comment relevant parts before running\n\n#### This notebook is an example of how you can load Watson Discovery Search Results into db2.  It will normalise the results into several tables, making it ideal to analyse within many BI tools such as Cognos Analytics", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Install Watson Developer Cloud", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!pip install pandas\n#!pip install numpy\n#!pip install --user pixiedust --upgrade\n!pip install -I watson-developer-cloud==0.26.1\n#print(\"Please restart the kernel and then run through the notebook again (skipping this cell).\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import requests\nimport json\nimport pandas as pd\nimport numpy as np\nfrom pandas.io.json import json_normalize\nfrom datetime import datetime\nimport time\nimport requests"
        }, 
        {
            "source": "#### Below is the method to sign into the Watson discovery Service.  In this example we are using the Watson Discovery News Selection.  However, the same method can be applied to custom collections.  The max doc count is 50 documents.  If you need to retrieve more, you can append multiple subsets of data to one dataset - as below.  Here, i have used the combination of count and offset to select the subset of documents to retrieve", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import sys\nimport os\nimport json\nfrom watson_developer_cloud import DiscoveryV1\n\nCrawlDate = str(datetime.date(datetime.today()))\n#CrawlDate = '2018-05-08'\nNLQ = '{NATURAL LANGUAGE QUERY}'  #REPLACE {NATURAL LANGUAGE QUERY} with your natural language search i.e Transport in London\nURLSearch = RBS.replace(\" \", \"%20\")\ndiscovery = DiscoveryV1(\n  username=\"{USERNAME}\", #REPLACE {USERNAME} with your username\n  password=\"{PASSWORD}\", #REPLACE {PASSWORD} with your password\n  version=\"2017-11-07\"\n)\n\n# USE THIS TO FILTER FOR EXAMPLE.....    'query': \"division:'PBB' ,\n\n#Below filter on news for today\nqopts = {'query': \"crawl_date:{},enriched_text.entities.text: {}\".format(CrawlDate, NLQ),'count':50, 'offset':0}\nqopts2 = {'query': \"crawl_date:{},enriched_text.entities.text: {}\".format(CrawlDate, NLQ),'count':50, 'offset':50}\nqopts3 = {'query': \"crawl_date:{},enriched_text.entities.text: {}\".format(CrawlDate, NLQ),'count':50, 'offset':100}\nqopts4 = {'query': \"crawl_date:{},enriched_text.entities.text: {}\".format(CrawlDate, NLQ),'count':50, 'offset':150}\n\n#Below filter on all news in the last 2 months\n#qopts = {'query': \"enriched_text.entities.text: {}\".format(NLQ),'count':50, 'offset':0}\n#qopts2 = {'query': \"enriched_text.entities.text: {}\".format(NLQ),'count':50, 'offset':50}\n#qopts3 = {'query': \"enriched_text.entities.text: {}\".format(NLQ),'count':50, 'offset':100}\n#qopts4 = {'query': \"enriched_text.entities.text: {}\".format(NLQ),'count':50, 'offset':150}\n\n"
        }, 
        {
            "source": "#### Below is the section where we retrieve the data.  System is the Watson Discovery Collection and News Environment is the Watson Discovery News Enviornment.  For custom collections, please amend as appropitate. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "newsz = discovery.query('system', 'news-en', qopts)\nnewsx = discovery.query('system', 'news-en', qopts2)\nnewsy = discovery.query('system', 'news-en', qopts3)\nnewsw = discovery.query('system', 'news-en', qopts4)\n"
        }, 
        {
            "source": "#### Below is the section where we package our data into data frames, then append all subsets of data into one dataframe.  We will finish with one dataframe which displays all the results ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "level1_df = pd.DataFrame()\n\nnewszdf = pd.io.json.json_normalize((newsz['results']),sep='_')\nnewsxdf = pd.io.json.json_normalize((newsx['results']),sep='_')\nnewsydf = pd.io.json.json_normalize((newsy['results']),sep='_')\nnewsvdf = pd.io.json.json_normalize((newsw['results']),sep='_')\n\nlevel1_df = level1_df.append(newszdf)\nlevel1_df = level1_df.append(newsxdf)\nlevel1_df = level1_df.append(newsydf)\nlevel1_df = level1_df.append(newsvdf)\n\n"
        }, 
        {
            "source": "#### Below is the section where we add additional columns to the dataframe - we reformat the publication date to a time based format as well.  I have then created a new dataframe which includes main index.  This is actually not required in this example, however, if you want to create multiple queries and use a 'FOR' statement to retrieve them all, you may want to have a new index but retain the original ones too - this is so you can join to a 'search masterfile' which would contain all searches", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Load all search results data into a dataframe\n\nlevel1 = pd.DataFrame()\nLevel1New = pd.DataFrame()\nLevelNew2 = pd.DataFrame()\n#ASSIGN LEVEL 1 HEADERS TO PLACES\n\n\nlevel1_df ['PublicationDate'] = level1_df.publication_date#.apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%SZ'))\nlevel1_df ['CrawlDate'] = level1_df.crawl_date.apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%SZ'))\nlevel1_df ['SearchQuery']= RBS    \nlevel1_df['PrimaryKey'] = level1_df.index\n    \nlevel1 = level1.append(level1_df)\n\n    \n    \nlevel1_df.reset_index(inplace=True)\n\n\nLevel1New = Level1New.append(level1)\nLevelNew2 = Level1New.reset_index()\nLevelNew2 ['PrimaryInd'] = LevelNew2.index\n\n"
        }, 
        {
            "source": "#### Here I am creating a new dataframe based on the existing one, then i am dropping columns that are not needed.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "##refine Header Table\n\n\nLevel3New = LevelNew2\nLevel3New = LevelNew2\nLevel3New = Level3New.drop((['enriched_text_categories',\n                             'enriched_text_concepts',\n                             'enriched_text_entities',\n                             'enriched_text_keywords',\n                             'enriched_text_relations',\n                             'enriched_text_semantic_roles',\n                             'enriched_title_categories',\n                             'enriched_title_entities',\n                             'enriched_title_keywords',\n                             'enriched_title_relations',\n                             'enriched_title_concepts',\n                             'crawl_date',\n                             'extracted_metadata_filename',\n                             'extracted_metadata_sha1',\n                             'enriched_title_semantic_roles'\n                            \n                            \n                            ]),1)\nLevel3New[['enriched_title_sentiment_document_score','enriched_text_sentiment_document_score']] = Level3New[['enriched_title_sentiment_document_score','enriched_text_sentiment_document_score']].apply(pd.to_numeric)\nLevel3New"
        }, 
        {
            "source": "#### Below, we are splitting the enriched columns into tables.  You will notice before that these columns had nested json.  This is how i deal with the results.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Detailed Analysis Tables\n\nLevel1New4 = pd.DataFrame(LevelNew2.index)\nConceptsHD = pd.DataFrame()\nCategoriesHD = pd.DataFrame()\nKeywordsHD = pd.DataFrame()\nLevel1Index = pd.DataFrame()\nentitiesHD = pd.DataFrame()\nSemanticHD = pd.DataFrame()\n\nConceptsHD = pd.DataFrame()\nLevel1Index = pd.DataFrame()\nLevel1Index ['ArrayIndex'] = Level1New4.index\nCONCEPTS = Level1Index.ArrayIndex\n\n#Concepts, categories, keywords, Entity and Relation Tables\n\nfor A in CONCEPTS:\n    ROOTIND = A\n    Concepts_Df = pd.io.json.json_normalize(LevelNew2.enriched_text_concepts[ROOTIND],sep='_') #the sep='_' is required because the default is a '.' seperator, which makes this difficult to query\n    Concepts_Df['PrimaryKey'] = A\n    Concepts_Df['ConceptKey']= Concepts_Df.index\n    ConceptsHD = ConceptsHD.append(Concepts_Df)\n  \n    \n    Categories_Df = pd.io.json.json_normalize(LevelNew2.enriched_text_categories[ROOTIND],sep='_')\n    Categories_Df['PrimaryKey'] = A\n    Categories_Df['CategoryKey']= Categories_Df.index\n    CategoriesHD = CategoriesHD.append(Categories_Df)\n \n    keywords_Df = pd.io.json.json_normalize(LevelNew2.enriched_text_keywords[ROOTIND],sep='_')\n    keywords_Df['PrimaryKey'] = A\n    keywords_Df['KeywordKey']= keywords_Df.index\n    KeywordsHD = KeywordsHD.append(keywords_Df)\n    \n    entities_Df = pd.io.json.json_normalize(LevelNew2.enriched_text_entities[ROOTIND],sep='_')\n    entities_Df['PrimaryKey'] = A\n    entities_Df['EntityKey']= entities_Df.index\n    entitiesHD = entitiesHD.append(entities_Df)\n    \n    Semantic_Df = pd.io.json.json_normalize(LevelNew2.enriched_text_semantic_roles[ROOTIND],sep='_')\n    Semantic_Df['PrimaryKey'] = A\n    Semantic_Df['SemanticKey']= Semantic_Df.index\n    SemanticHD = SemanticHD.append(Semantic_Df)\n\nConcepts_Df.reset_index(inplace=True)    \nCategories_Df.reset_index(inplace=True)\nkeywords_Df.reset_index(inplace=True)\nentities_Df.reset_index(inplace=True)\nSemantic_Df.reset_index(inplace=True)\n\nCategories_Table = CategoriesHD.reset_index()\nConcepts_Table = ConceptsHD.reset_index()\nKeywords_Table = KeywordsHD.reset_index()\nEntities_Table = entitiesHD.reset_index()\nSemantic_Table = SemanticHD.reset_index()"
        }, 
        {
            "source": "#### Here i am viewing the results of the sub tables", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Concepts_Table.head(10)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Categories_Table.head(5)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Keywords_Table.head(1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Entities_Table.head(3)"
        }, 
        {
            "source": "#### Here i am dropping unnessary Semantic Table columns, and creating new dataframes for the nested json within Semantic_Table.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "SemantecEntities = pd.DataFrame(Semantic_Table.object_entities).dropna (axis=0, how='all')\nSemantecKeywords = pd.DataFrame(Semantic_Table.object_keywords).dropna (axis=0, how='all')\nSemantic_Table2 = pd.DataFrame(Semantic_Table.drop((['object_entities','object_keywords','subject_keywords','subject_entities','subject_text']),1))\nSemantic_Table2 ['PrimaryInd'] = Semantic_Table2.index                               \nSemantic_Table2"
        }, 
        {
            "source": "#### This is how we populate new dataframes with the nested json held in the Semantic Table - Semantic Keywords and Semantic Entities", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Detailed Semantic Tables\n\nObjectEntitiesHD = pd.DataFrame()\nObjectKeywordsHD = pd.DataFrame()\nSemanticIndex = pd.DataFrame()\nSemanticIndex ['ArrayIndex'] = SemantecEntities.index\nSemanticIndex1 = pd.DataFrame()\nSemanticIndex1 ['ArrayIndex'] = SemantecKeywords.index\n\nSEMANTICDET = SemanticIndex.ArrayIndex\n\nSEMANTICDET1 = SemanticIndex1.ArrayIndex\n\n#ObjectEntities and Keywords\n\nfor A in SEMANTICDET:\n    SEMIND = A\n    ObjectEntities_Df = pd.io.json.json_normalize(Semantic_Table.object_entities[SEMIND],sep='_')\n    ObjectEntities_Df['Semantic_Key'] = A\n    #ObjectEntities_Df['ObjectEnt_Key']= ObjectEntities_Df.index\n    ObjectEntitiesHD = ObjectEntitiesHD.append(ObjectEntities_Df)\n    \n                                                  \nObjectEntities_Df.reset_index(inplace=True)                                                  \nSemanticsObjectEntitiesTable = ObjectEntitiesHD.reset_index()                                                  \n\nfor B in SEMANTICDET1:\n    SEMIND1 = B\n    ObjectKeywords_Df = pd.io.json.json_normalize(Semantic_Table.object_keywords[SEMIND1],sep='_')\n    ObjectKeywords_Df['Semantic_Key'] = B\n    ObjectKeywords_Df['Object_Key']= ObjectKeywords_Df.index\n    ObjectKeywordsHD = ObjectKeywordsHD.append(ObjectKeywords_Df)\n        \nObjectKeywords_Df.reset_index(inplace=True)\nSemanticsKeywordsTable = ObjectKeywordsHD.reset_index()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "SemanticsObjectEntitiesTable = SemanticsObjectEntitiesTable.drop(('disambiguation_subtype'),1)\n\n\n\nSemanticsObjectEntitiesTable.head(2)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "SemanticsKeywordsTable.head(2)"
        }, 
        {
            "source": "#### Here i have chosen to apply the same numeric number to every row in every table which is imported.  This is so i can later query on the batch that has been imported.  If you are updating tables with new data, you may want to delete the entire batch - this timestamp will allow you to do this in SQL", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Add a timestamp to all tables\nfrom datetime import datetime\nimport time\nimport requests\nDateTime = datetime.today().strftime(\"%d%m%Y%H%M%S\")\nSemanticsKeywordsTable['TimeStamp'] = DateTime \n\nSemanticsObjectEntitiesTable['TimeStamp'] = DateTime \n\nSemantic_Table2['TimeStamp'] = DateTime\nEntities_Table['TimeStamp'] = DateTime\nKeywords_Table['TimeStamp'] = DateTime\nCategories_Table['TimeStamp'] = DateTime\nConcepts_Table['TimeStamp'] = DateTime\nLevel3New['TimeStamp'] = DateTime\n\nSemantic_Table2.dtypes"
        }, 
        {
            "source": "#### Here i am adding a publication date and time based on the date and time field", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from datetime import datetime\nimport time\n\nLevel3New ['Publication_Date_and_Time'] = Level3New.publication_date.str.slice(0,19).apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S'))\nLevel3New ['Publication_Date'] = Level3New.publication_date.str.slice(0, 10)\nLevel3New ['Publication_Time'] = Level3New.publication_date.str.slice(12, 19)\n\n#Level3New['PrimaryKey'] = level3New.index\n\nLevel3New.head(20000)"
        }, 
        {
            "source": "#### This is where we reconstruct the header table - also to include additional columns such as YEAR, MONTH, DAY, MONTH NAME, WEEKDAY NUMBER AND WEEKDAY NAME.  Please note, you will see that the text is a substring (1st 200 characters) - the limit you can enter will depend on the column size of the field in db2.  if ibmdbpy automatically generates the tables, it will create tables based on information given from the first loaded record.  I would advise to load (autogenerate) initially to retrieve the tables, amend the suggested column sizes in db2 then delete all rows,  and then from that point onwards use the append section when you load actual data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import calendar\nimport datetime\n\nHEADER = Level3New\n\nHEADER.text = HEADER.text.str.slice(0, 600)\n\nHEADER_2 = pd.DataFrame()\nHEADER_2['PRIMARY_KEY']= HEADER.PrimaryKey\nHEADER_2['DocID']= HEADER.id\nHEADER_2['PUBLICATION_DATE']= HEADER.Publication_Date\nHEADER_2['YEAR'] = HEADER.Publication_Date_and_Time.dt.year\nHEADER_2['MONTH'] = HEADER.Publication_Date_and_Time.dt.month\nHEADER_2['DAY'] = HEADER.Publication_Date_and_Time.dt.day\nHEADER_2['MONTH_NAME'] =  HEADER_2['MONTH'].astype(str).str.zfill(2) + (' ') + HEADER_2['MONTH'].apply(lambda x: calendar.month_name[x])\nHEADER_2['MONTH_NME'] =  HEADER_2['MONTH'].apply(lambda x: calendar.month_name[x])\nHEADER_2['WEEKDAY_NUMBER'] = HEADER.Publication_Date_and_Time.dt.weekday\nHEADER_2['WEEKDAY_NAME'] = HEADER_2['WEEKDAY_NUMBER'].apply(lambda x: calendar.day_name[x])\nHEADER_2['Month_Number Str'] = HEADER_2['MONTH'].astype(str).str.zfill(2)\nHEADER_2['PUBLICATION_DATE_AND_TIME']= HEADER.Publication_Date_and_Time\nHEADER_2['author']= HEADER.author\nHEADER_2['URL']= HEADER.url\nHEADER_2['Title']= HEADER.title\nHEADER_2['Forum Title']= HEADER.forum_title\nHEADER_2['Source Typ']= HEADER.source_type\nHEADER_2['Country']= HEADER.country\nHEADER_2['text']= HEADER.text.str.slice(0, 200)\nHEADER_2['Sentiment_Score']= HEADER.enriched_text_sentiment_document_score\nHEADER_2['TimeStamp']= HEADER.TimeStamp\nHEADER_2.head(10)#.text[0]"
        }, 
        {
            "source": "#### Here we are reconstructing the Entities table and slicing the text to add the first 200 characters - again, here you could do an initial load, alter the table properties for the text column and then reload using the update method ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Entities_Table2 = pd.DataFrame()\n\nEntities_Table2['index'] = Entities_Table.index\n#Entities_Table2['Entity_Key'] = Entities_Table.EntityKey\nEntities_Table2['Primary_Key'] = Entities_Table.PrimaryKey                           \n#Entities_Table2['Count'] = Entities_Table.count                              \nEntities_Table2['disambiguation_resource'] = Entities_Table.disambiguation_dbpedia_resource     \nEntities_Table2['disambiguation_name'] = Entities_Table.disambiguation_name\nEntities_Table2['disambiguation_subtype'] = Entities_Table.disambiguation_subtype.str[0]\nEntities_Table2['relevance'] = Entities_Table.relevance                          \nEntities_Table2['sentiment_label'] = Entities_Table.sentiment_label                     \nEntities_Table2['sentiment_score'] = Entities_Table.sentiment_score                    \nEntities_Table2['text'] = Entities_Table.text.str.slice(0,200)                                \nEntities_Table2['type'] = Entities_Table.type                                \nEntities_Table2['TimeStamp'] = Entities_Table.TimeStamp\n\nEntities_Table2.head(5)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [], 
            "source": "Semantic_Table2.object_text = Semantic_Table2.object_text.str.slice(0, 200)\nSemantic_Table2.sentence = Semantic_Table2.sentence.str.slice(0, 200)\n#Semantic_Table2.subject_text = Semantic_Table2.subject_text.str.slice(0, 200)\n\nSemantic_Table2"
        }, 
        {
            "source": "#### To ensure the order of columns is the same as what is in the database, i have set the order into new dataframes (with the exeption of entities and Header which have already been reconstructed)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Categories_TableDB2 = pd.DataFrame()\nSemantic_TableDB2 = pd.DataFrame()\nConcepts_TableDB2 = pd.DataFrame()\nKeywords_TableDB2 = pd.DataFrame()\n\n#Reorder Categories Table\n\nCategories_TableDB2['index'] = Categories_Table.index\nCategories_TableDB2['label'] = Categories_Table.label\nCategories_TableDB2['score'] = Categories_Table.score\nCategories_TableDB2['PrimaryKey'] = Categories_Table.PrimaryKey\nCategories_TableDB2['CategoryKey'] = Categories_Table.CategoryKey\nCategories_TableDB2['TimeStamp'] = Categories_Table.TimeStamp\n\n\n#Reorder Concepts Table\n\nConcepts_TableDB2['index'] = Concepts_Table.index\nConcepts_TableDB2['dbpedia_resource'] = Concepts_Table.dbpedia_resource\nConcepts_TableDB2['relevance'] = Concepts_Table.relevance\nConcepts_TableDB2['text'] = Concepts_Table.text\nConcepts_TableDB2['PrimaryKey'] = Concepts_Table.PrimaryKey\nConcepts_TableDB2['ConceptKey'] = Concepts_Table.ConceptKey\nConcepts_TableDB2['TimeStamp'] = Concepts_Table.TimeStamp\n\n#Reorder Semantic Table\n\nSemantic_TableDB2['index'] = Semantic_Table2.index\nSemantic_TableDB2['PrimaryKey'] = Semantic_Table2.PrimaryKey\nSemantic_TableDB2['SemanticKey'] = Semantic_Table2.SemanticKey\nSemantic_TableDB2['action_normalized'] = Semantic_Table2.action_normalized\nSemantic_TableDB2['action_text'] = Semantic_Table2.action_text\nSemantic_TableDB2['action_verb_negated'] = Semantic_Table2.action_verb_negated\nSemantic_TableDB2['action_verb_tense'] = Semantic_Table2.action_verb_tense\nSemantic_TableDB2['action_verb_text'] = Semantic_Table2.action_verb_text\nSemantic_TableDB2['object_text'] = Semantic_Table2.object_text\nSemantic_TableDB2['sentence'] = Semantic_Table2.sentence\nSemantic_TableDB2['PrimaryInd'] = Semantic_Table2.PrimaryInd\nSemantic_TableDB2['TimeStamp'] = Semantic_Table2.TimeStamp\n\n#Reorder Keywords Table\n\nKeywords_TableDB2['index']= Keywords_Table.index\nKeywords_TableDB2['relevance']= Keywords_Table.relevance\nKeywords_TableDB2['sentiment_label']= Keywords_Table.sentiment_label\nKeywords_TableDB2['sentiment_score']= Keywords_Table.sentiment_score\nKeywords_TableDB2['text']= Keywords_Table.text\nKeywords_TableDB2['PrimaryKey']= Keywords_Table.PrimaryKey\nKeywords_TableDB2['KeywordKey']= Keywords_Table.KeywordKey\nKeywords_TableDB2['TimeStamp']= Keywords_Table.TimeStamp\n"
        }, 
        {
            "source": "### LOAD DATA INTO DB2, the first section updates existing tables, and the 2nd section Drops and Recreates tables based on the dataframe properties  Use second section for inital load and first section for updates (which is the one you would use for ongoing document retrieval", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import ibmdbpy\nfrom ibmdbpy import IdaDataBase\n\n#REPLACE HOSTNAME, USERNAME AND PASSWORD WITH your db2 credentials\n\nidadb = IdaDataBase(dsn='DASHDB;Database=BLUDB;Hostname={HOSTNAME};Port=50000;PROTOCOL=TCPIP;UID={USERNAME};PWD={PASSWORD}',autocommit=True, verbose=False)\n\n#  1ST SECTION USE THIS TO UPDATE TABLES (un comment when you use this)\n\n\n#NEWS_KEYWORDS_TABLE = IdaDataFrame(idadb, 'BLUADMIN.NEWS_KEYWORDS_TABLE')\n#NEWS_CATEGORIES_TABLE = IdaDataFrame(idadb, 'BLUADMIN.NEWS_CATEGORIES_TABLE')\n#NEWS_SEMANTIC_TABLE = IdaDataFrame(idadb, 'BLUADMIN.NEWS_SEMANTIC_TABLE')\n#NEWS_ENTITIES_TABLE = IdaDataFrame(idadb, 'BLUADMIN.NEWS_ENTITIES_TABLE')\n#NEWS_CONCEPTS_TABLE = IdaDataFrame(idadb, 'BLUADMIN.NEWS_CONCEPTS_TABLE')\n#NEWS_HEADER_TABLE = IdaDataFrame(idadb, 'BLUADMIN.NEWS_HEADER_TABLE')\n\n#idadb.append(NEWS_KEYWORDS_TABLE, Keywords_TableDB2, maxnrow=None)\n#idadb.append(NEWS_CATEGORIES_TABLE, Categories_TableDB2, maxnrow=None)\n#idadb.append(NEWS_SEMANTIC_TABLE, Semantic_TableDB2, maxnrow=None)\n#idadb.append(NEWS_ENTITIES_TABLE, Entities_Table2, maxnrow=None)\n#idadb.append(NEWS_CONCEPTS_TABLE, Concepts_TableDB2, maxnrow=None)\n#idadb.append(NEWS_HEADER_TABLE, HEADER_2, maxnrow=None)\n\n\n########################################################################################\n\n# 2ND SECTION USE THIS TO DROP AND REBUILD TABLES - NOTE ORIGINAL TABLES WILL BE DELETED (uncomment when you use this)\n\n#idadf1 = idadb.as_idadataframe(Keywords_Table, \"NEWS_KEYWORDS_TABLE\", clear_existing=True)\n#idadf2 = idadb.as_idadataframe(Categories_Table, \"NEWS_CATEGORIES_TABLE\", clear_existing=True)\n#idadf8 = idadb.as_idadataframe(Semantic_Table2, \"NEWS_SEMANTIC_TABLE\", clear_existing=True)\n#idadf9 = idadb.as_idadataframe(Entities_Table2, \"NEWS_ENTITIES_TABLE\", clear_existing=True)\n#idadf4 = idadb.as_idadataframe(Concepts_Table, \"NEWS_CONCEPTS_TABLE\", clear_existing=True)\n#idadf7 = idadb.as_idadataframe(HEADER_2, \"NEWS_HEADER_TABLE\", clear_existing=True)\n\n########################################################################################\n\n\n#Current WIP - This section still needs refinement\n#idadf3 = idadb.as_idadataframe(SemanticsObjectEntitiesTable, \"NEWS_OBJ_ENTITIES_TABLE\", clear_existing=True)\n#idadf5 = idadb.as_idadataframe(SemanticsKeywordsTable, \"NEWS_OBJ_KEYWORDS_TABLE\", clear_existing=True)\n\nidadb.close()"
        }, 
        {
            "source": "# Tables in db2", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "IdaDataFrame(idadb, 'BLUADMIN.NEWS_KEYWORDS_TABLE').dtypes#head()\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "IdaDataFrame(idadb, 'BLUADMIN.NEWS_CATEGORIES_TABLE').dtypes#head()\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "Categories_TableDB2#head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "IdaDataFrame(idadb, 'BLUADMIN.NEWS_SEMANTIC_TABLE').dtypes#head()\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "IdaDataFrame(idadb, 'BLUADMIN.NEWS_ENTITIES_TABLE').dtypes#head()\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "IdaDataFrame(idadb, 'BLUADMIN.NEWS_CONCEPTS_TABLE').dtypes#head()\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "IdaDataFrame(idadb, 'BLUADMIN.NEWS_HEADER_TABLE').dtypes#head()"
        }, 
        {
            "source": "#### please contact becky.oconnor@uk.ibm.com if you have any questions on the above.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 1.6 (Unsupported)", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}